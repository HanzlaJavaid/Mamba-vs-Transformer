{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcr3Q0efdTUv",
        "outputId": "fcaf3998-b67c-4a3f-9837-9ed941d9c044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m554.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio -q\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html -q\n",
        "!pip install mamba-ssm==1.2.0.post1 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_EEESZK4dO52",
        "outputId": "057f72b1-08e2-492c-b7f3-e35c7916f1e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9b413f3aeb7eb90f60.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://9b413f3aeb7eb90f60.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "import gradio as gr\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Transformer-based model setup\n",
        "# ------------------------------------------------------------\n",
        "transformer_model_path = \"hanzla/bert-essay-classifier\"\n",
        "transformer_model = AutoModelForSequenceClassification.from_pretrained(transformer_model_path)\n",
        "transformer_tokenizer = AutoTokenizer.from_pretrained(transformer_model_path)\n",
        "transformer_labels = [\"Human Written\", \"AI Written\"]\n",
        "\n",
        "def classify_text_transformer(text):\n",
        "    start_time = time.time()\n",
        "    inputs = transformer_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = transformer_model(**inputs)\n",
        "    predictions = torch.softmax(outputs.logits, dim=1)\n",
        "    probabilities = predictions.detach().numpy().flatten() * 100\n",
        "    computation_time = time.time() - start_time\n",
        "\n",
        "    # Format results as a Markdown table\n",
        "    table_str = \"| Class | Probability |\\n|---|---|\\n\"\n",
        "    for i, label in enumerate(transformer_labels):\n",
        "        table_str += f\"| {label} | {probabilities[i]:.2f}% |\\n\"\n",
        "\n",
        "    result = (\n",
        "        f\"**Computation Time:** {computation_time:.4f} seconds\\n\\n\"\n",
        "        \"**Prediction Probabilities:**\\n\\n\" + table_str +\n",
        "        f\"\\n**Predicted Class:** `{transformer_labels[probabilities.argmax()]}`\"\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# MAMBA-based model setup\n",
        "# ------------------------------------------------------------\n",
        "class MambaTextClassifier:\n",
        "    def __init__(self, model_path, device=None):\n",
        "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Initialize base model\n",
        "        self.model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-370m\")\n",
        "        self.model.lm_head = nn.Linear(self.model.config.d_model, 2)\n",
        "\n",
        "        # Load weights\n",
        "        state_dict = torch.load(model_path, map_location=self.device)\n",
        "        if 'state_dict' in state_dict:\n",
        "            state_dict = state_dict['state_dict']\n",
        "\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.labels = ['Human-written', 'AI-generated']\n",
        "\n",
        "    def classify_text(self, text):\n",
        "        start_time = time.time()\n",
        "\n",
        "        encoded_input = self.tokenizer(\n",
        "            text,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(encoded_input[\"input_ids\"])\n",
        "            logits = outputs.logits[:, -1, :]  # last token logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "        end_time = time.time()\n",
        "        computation_time = end_time - start_time\n",
        "\n",
        "        probabilities = probabilities.cpu().numpy().flatten() * 100\n",
        "\n",
        "        table_str = \"| Class | Probability |\\n|---|---|\\n\"\n",
        "        for i, label in enumerate(self.labels):\n",
        "            table_str += f\"| {label} | {probabilities[i]:.2f}% |\\n\"\n",
        "\n",
        "        results = (\n",
        "            f\"**Computation Time:** {computation_time:.4f} seconds\\n\\n\"\n",
        "            \"**Prediction Probabilities:**\\n\\n\" + table_str +\n",
        "            f\"\\n**Predicted Class:** `{self.labels[probabilities.argmax()]}`\"\n",
        "        )\n",
        "        return results\n",
        "\n",
        "mamba_classifier = MambaTextClassifier(\"drive/MyDrive/mamba_model/mamba_ai_essay_detection_model_balanced.pth\")\n",
        "\n",
        "def classify_text_mamba(text):\n",
        "    return mamba_classifier.classify_text(text)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Combined function\n",
        "# ------------------------------------------------------------\n",
        "def classify_texts(text):\n",
        "    transformer_result = classify_text_transformer(text)\n",
        "    mamba_result = classify_text_mamba(text)\n",
        "    return transformer_result, mamba_result\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Gradio Interface\n",
        "# ------------------------------------------------------------\n",
        "with gr.Blocks(theme=gr.themes.Default()) as demo:\n",
        "    # Header\n",
        "    gr.Markdown(\"<h1 style='text-align: center; margin-bottom: 1em;'>AI vs. Human-Written Text Classifier</h1>\", elem_id=\"title\")\n",
        "    gr.Markdown(\"<p style='text-align: center; font-size:1.1em;'>Compare the predictions of two different models to determine if text is AI-generated or human-written.</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            text_input = gr.Textbox(label=\"Input Text\", lines=5, placeholder=\"Enter the text you want to classify here...\")\n",
        "            with gr.Row():\n",
        "                submit_btn = gr.Button(\"Classify\", variant=\"primary\")\n",
        "                example_btn = gr.Button(\"Try an Example\")\n",
        "\n",
        "        with gr.Column():\n",
        "            # Results area\n",
        "            with gr.Tab(\"Transformer Model\"):\n",
        "                transformer_output = gr.Markdown(label=\"Transformer Model Results\")\n",
        "            with gr.Tab(\"MAMBA Model\"):\n",
        "                mamba_output = gr.Markdown(label=\"MAMBA Model Results\")\n",
        "\n",
        "    def load_example():\n",
        "        return \"\"\"\n",
        "          Nearly ten years had passed since the Dursleys had woken up to find their nephew on the front step, but Privet Drive had hardly changed at all. The sun rose on the same tidy front gardens and lit up the brass number four on the Dursleys' front door; it crept into their living room, which was almost exactly the same as it had been on the night when Mr. Dursley had seen that fateful news report about the owls. Only the photographs on the mantelpiece really showed how much time had passed. Ten years ago, there had been lots of pictures of what looked like a large pink beach ball wearing different-colored bonnets - but Dudley Dursley was no longer a baby, and now the photographs showed a large blond boy riding his first bicycle, on a carousel at the fair, playing a computer game with his father, being hugged and kissed by his mother. The room held no sign at all that another boy lived in the house, too.\n",
        "\n",
        "          Yet Harry Potter was still there, asleep at the moment, but not for long. His Aunt Petunia was awake and it was her shrill voice that made the first noise of the day.\n",
        "\n",
        "          \"Up! Get up! Now!\"\n",
        "\n",
        "          Harry woke with a start. His aunt rapped on the door again.\n",
        "\n",
        "          \"Up!\" she screeched. Harry heard her walking toward the kitchen and then the sound of the frying pan being put on the stove. He rolled onto his back and tried to remember the dream he had been having. It had been a good one. There had been a flying motorcycle in it. He had a funny feeling he'd had the same dream before.\n",
        "\n",
        "          His aunt was back outside the door.\n",
        "\n",
        "          \"Are you up yet?\" she demanded.\n",
        "\n",
        "          \"Nearly,\" said Harry.\n",
        "\n",
        "          \"Well, get a move on, I want you to look after the bacon. And don't you dare let it burn, I want everything perfect on Duddy's birthday.\"\n",
        "\n",
        "          Harry groaned.\n",
        "\n",
        "          \"What did you say?\" his aunt snapped through the door.\n",
        "\n",
        "          \"Nothing, nothing . . .\"\n",
        "\n",
        "          Dudley's birthday - how could he have forgotten? Harry got slowly out of bed and started looking for socks. He found a pair under his bed and, after pulling a spider off one of them, put them on. Harry was used to spiders, because the cupboard under the stairs was full of them, and that was where he slept.\n",
        "\n",
        "          When he was dressed he went down the hall into the kitchen. The table was almost hidden beneath all Dudley's birthday presents. It looked as though Dudley had gotten the new computer he wanted, not to mention the second television and the racing bike. Exactly why Dudley wanted a racing bike was a mystery to Harry, as Dudley was very fat and hated exercise - unless of course it involved punching somebody. Dudley's favorite punching bag was Harry, but he couldn't often catch him. Harry didn't look it, but he was very fast.\n",
        "\n",
        "          Perhaps it had something to do with living in a dark cupboard, but Harry had always been small and skinny for his age. He looked even smaller and skinnier than he really was because all he had to wear were old clothes of Dudley's, and Dudley was about four times bigger than he was. Harry had a thin face, knobbly knees, black hair, and bright green eyes. He wore round glasses held together with a lot of Scotch tape because of all the times Dudley had punched him on the nose. The only thing Harry liked about his own appearance was a very thin scar on his forehead that was shaped like a bolt of lightning. He had had it as long as he could remember, and the first question he could ever remember asking his Aunt Petunia was how he had gotten it.\n",
        "\n",
        "          \"In the car crash when your parents died,\" she had said. \"And don't ask questions.\"\n",
        "\n",
        "          Don't ask questions - that was the first rule for a quiet life with the Dursleys.\n",
        "\n",
        "          Uncle Vernon entered the kitchen as Harry was turning over the bacon.\n",
        "\n",
        "          \"Comb your hair!\" he barked, by way of a morning greeting.\n",
        "          \"\"\"\n",
        "\n",
        "    example_btn.click(fn=load_example, inputs=[], outputs=[text_input])\n",
        "    submit_btn.click(\n",
        "        fn=classify_texts,\n",
        "        inputs=[text_input],\n",
        "        outputs=[transformer_output, mamba_output]\n",
        "    )\n",
        "\n",
        "    # Footer or additional info in an accordion\n",
        "    with gr.Accordion(\"Additional Information\"):\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            **Note:**\n",
        "            - The predictions are probabilistic.\n",
        "            - The models have been trained on certain datasets and may not be accurate for all types of text.\n",
        "            - Computation time may vary based on your hardware.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udbc8rwLfmgP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}